{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Classifier Training",
   "id": "7f289f6059cdd0ec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Initial Setup",
   "id": "91e82f85ef849e31"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T12:12:40.990437Z",
     "start_time": "2024-10-22T12:12:40.844548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as functional\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "import gc\n",
    "from torchvision.models.resnet import resnet18\n",
    "from torchsummary import summary\n",
    "from tqdm.auto import tqdm\n",
    "import torch_directml\n",
    "from dataset import TEST_DATASET, TRAIN_DATASET, get_elements_from_indexes, LABELS\n",
    "from datasets import load_dataset\n",
    "from collections import OrderedDict\n",
    "from segmentation.k_means import kmeans, KmeansFlags, KmeansTermCrit, KmeansTermOpt\n",
    "from improving.filtering import conv2d\n",
    "import cv2\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "import seaborn as sns"
   ],
   "id": "7e3ef6710a8c056c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# MatPlotLib Configuration\n",
    "%matplotlib notebook\n",
    "plt.ioff()"
   ],
   "id": "203c1d32c3000961"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Set device\n",
    "dml = torch_directml.device()"
   ],
   "id": "a3690e3c51f74a92"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Functions\n",
    "# Plot single-channel images with a fixed configuration\n",
    "def draw_grayscale_image(image, ax):\n",
    "    ax.imshow(image, cmap='gray', vmin=0, vmax=255)\n",
    "    ax.axis('off')\n",
    "    \n",
    "# Clear output and then execute decorator\n",
    "def clear_and_execute(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        clear_output()\n",
    "        return func(*args, **kwargs)\n",
    "    return wrapper\n",
    "\n",
    "# Image pre-processing\n",
    "# Filter definition\n",
    "epsilon = 0.1\n",
    "sigma = 3\n",
    "gaussianDim = int(np.ceil(np.sqrt(-2 * sigma ** 2 * np.log(epsilon * sigma * np.sqrt(2 * np.pi)))))\n",
    "gaussianKernel1D = cv2.getGaussianKernel(gaussianDim, sigma)\n",
    "gaussianKernel = np.outer(gaussianKernel1D, gaussianKernel1D)\n",
    "@clear_and_execute\n",
    "def process_element(element):\n",
    "    image = element[0]\n",
    "    label = element[1]\n",
    "    filtered_image = conv2d(image, gaussianKernel)\n",
    "    compactness, labels, centers = kmeans(\n",
    "        filtered_image.flatten(), 3, \n",
    "        criteria=KmeansTermCrit(KmeansTermOpt.BOTH, 20, 0.5),\n",
    "        flags=KmeansFlags.KMEANS_PP_CENTERS, attempts=5\n",
    "    )\n",
    "    centers = centers.astype(np.uint8)\n",
    "    segmented_kmeans = centers[labels].reshape(image.shape)\n",
    "    sorted_centers = sorted(centers)\n",
    "    white_matter_idx = np.argmax(centers == sorted_centers[2])\n",
    "    grey_matter_idx = np.argmax(centers == sorted_centers[1])\n",
    "    segmented_white_matter = np.where(segmented_kmeans == centers[white_matter_idx], 1, 0).astype(np.uint8)\n",
    "    segmented_grey_matter = np.where(segmented_kmeans == centers[grey_matter_idx], 1, 0).astype(np.uint8)\n",
    "    return np.array((image, segmented_white_matter, segmented_grey_matter)), label\n",
    "\n",
    "# Parallel processing of images\n",
    "def parallel_processing(dataset_elements):\n",
    "    dataset_length = len(dataset_elements)\n",
    "    data = np.zeros((dataset_length, 3, *img_shape), dtype=np.uint8)\n",
    "    labels = np.zeros((dataset_length,), dtype=np.uint8)\n",
    "    results = Parallel(n_jobs=8)(delayed(process_element)(element) for element in tqdm(dataset_elements))\n",
    "    for index in range(len(results)):\n",
    "        data[index] = results[index][0]\n",
    "        labels[index] = results[index][1]\n",
    "    return data, labels.astype(np.uint8)"
   ],
   "id": "4bf3ad9112a85f68"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T02:15:18.437961Z",
     "start_time": "2024-10-17T02:15:18.434408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Pre-processed dataset files\n",
    "if not os.path.exists('pre_processed_dataset'):\n",
    "    os.mkdir('pre_processed_dataset')\n",
    "TRAIN_DATA_FILE = 'pre_processed_dataset/train.npy'\n",
    "TRAIN_LABELS_FILE = 'pre_processed_dataset/train_labels.npy'\n",
    "TEST_DATA_FILE = 'pre_processed_dataset/test.npy'\n",
    "TEST_LABELS_FILE = 'pre_processed_dataset/test_labels.npy'"
   ],
   "id": "aba60ca9e7cb4a3c",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Checkpoint folder creation\n",
    "if not os.path.exists('checkpoints'):\n",
    "    os.mkdir('checkpoints')"
   ],
   "id": "e0ddfb0819070709"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset Preprocessing",
   "id": "29cad924978028d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_dataset = load_dataset(**TRAIN_DATASET)\n",
    "train_dataset_length = len(train_dataset)\n",
    "train_dataset_elements = get_elements_from_indexes(train_dataset, np.arange(len(train_dataset)))\n",
    "img_shape = train_dataset_elements[0][0].shape\n",
    "test_dataset = load_dataset(**TEST_DATASET)\n",
    "test_dataset_elements = get_elements_from_indexes(test_dataset, np.arange(len(test_dataset)))"
   ],
   "id": "9758bcd772f6149b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Train Data Processing\n",
    "train_data, train_labels = parallel_processing(train_dataset_elements)\n",
    "print(\"Saving training data\")\n",
    "np.save(TRAIN_DATA_FILE, train_data)\n",
    "np.save(TRAIN_LABELS_FILE, train_labels)"
   ],
   "id": "17835d9fff2b72c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Test Data Processing\n",
    "test_data, test_labels = parallel_processing(test_dataset_elements)\n",
    "print(\"Saving testing data\")\n",
    "np.save(TEST_DATA_FILE, test_data)\n",
    "np.save(TEST_LABELS_FILE, test_labels)"
   ],
   "id": "4236eff51805d2d0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Validate the results\n",
    "train_data = np.load(TRAIN_DATA_FILE)\n",
    "train_labels = np.load(TRAIN_LABELS_FILE)\n",
    "random_index = np.random.randint(train_data.shape[0])\n",
    "sample = train_data[random_index]\n",
    "sample_image, sample_white_matter, sample_grey_matter = sample[0], sample[1], sample[2]\n",
    "sample_label = train_labels[random_index]\n",
    "if 'sample_fig' in globals():\n",
    "    plt.close('Sample image')\n",
    "sample_fig = plt.figure(figsize=(6, 2.3), num=\"Sample Image\")\n",
    "sample_axs = sample_fig.subplots(1, 3)\n",
    "draw_grayscale_image(sample_image, sample_axs[0])\n",
    "draw_grayscale_image(sample_white_matter * 255, sample_axs[1])\n",
    "draw_grayscale_image(sample_grey_matter * 255, sample_axs[2])\n",
    "sample_fig.suptitle(f\"Example Figure, label: {LABELS[sample_label]}\")\n",
    "sample_fig.tight_layout()\n",
    "sample_fig.show()"
   ],
   "id": "ec83307156a22644"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset Handlers & Dataloaders",
   "id": "6b34380dd410c86"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Dataset definition\n",
    "class DatasetHandler(Dataset):\n",
    "    def __init__(self, path_to_data: str, path_to_labels: str):\n",
    "        self.data = torch.tensor(np.load(path_to_data), dtype=torch.float)\n",
    "        self.data[:,0] /= 255 # Must be divided to be between 0 and 1\n",
    "        labels = np.load(path_to_labels)\n",
    "        self.labels = torch.zeros((len(labels), 4))\n",
    "        for index in range(len(labels)):\n",
    "            self.labels[index][labels[index]] = 1\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ],
   "id": "e80a5c1ea8b097c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Creation of dataloaders\n",
    "train_dataset_handler = DatasetHandler(TRAIN_DATA_FILE, TRAIN_LABELS_FILE)\n",
    "test_dataset_handler = DatasetHandler(TEST_DATA_FILE, TEST_LABELS_FILE)\n",
    "batch_size_train = 16\n",
    "batch_size_test = 32\n",
    "n_workers = 4\n",
    "train_dataloader = DataLoader(train_dataset_handler, batch_size=batch_size_train, shuffle=True,\n",
    "                              num_workers=0, pin_memory=False)\n",
    "test_dataloader = DataLoader(test_dataset_handler, batch_size=batch_size_test, shuffle=True,\n",
    "                             num_workers=0, pin_memory=False)"
   ],
   "id": "39518b60c0d7660d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Module Initialization Functions",
   "id": "366e48020157b563"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def init_weights(net, init='norm', gain=0.02, verbose: bool = True):\n",
    "    def init_func(m):\n",
    "        classname = m.__class__.__name__\n",
    "        if hasattr(m, 'weight') and 'Conv' in classname:\n",
    "            if init == 'norm':\n",
    "                nn.init.normal_(m.weight.validation_data, mean=0.0, std=gain)\n",
    "            elif init == 'xavier':\n",
    "                nn.init.xavier_normal_(m.weight.validation_data, gain=gain)\n",
    "            elif init == 'kaiming':\n",
    "                nn.init.kaiming_normal_(m.weight.validation_data, a=0, mode='fan_in')\n",
    "            if hasattr(m, 'bias') and m.bias is not None:\n",
    "                nn.init.constant_(m.bias.validation_data, 0.0)\n",
    "        elif 'BatchNorm2d' in classname:\n",
    "            nn.init.normal_(m.weight.validation_data, 1., gain)\n",
    "            nn.init.constant_(m.bias.validation_data, 0.)\n",
    "    net.apply(init_func)\n",
    "    if verbose:\n",
    "        print(f\"Model initialized with {init} initialization.\")\n",
    "    return net\n",
    "\n",
    "\n",
    "def init_model(model):\n",
    "    model = init_weights(model)\n",
    "    return model"
   ],
   "id": "a3b915479a2d7123"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Architecture",
   "id": "838eb1b7c28a76aa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ResNet18",
   "id": "cbeb4b24f04d71fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## ResNet Constitution\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, downsample=False):\n",
    "        # In the init we declare the attributes of the block (mostly layers)\n",
    "        super().__init__()\n",
    "        self.downsample = downsample\n",
    "        first_stride = 2 if downsample else 1\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=first_stride, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu1 = nn.ReLU(inplace=False)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False) \n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu2 = nn.ReLU(inplace=False)\n",
    "        if downsample:\n",
    "            # at the end of the residual block, we add the input to the output (skip connection)\n",
    "            # but if we reduce the resolution of the output (with stride 2) the resolutions don't match\n",
    "            # so we also have to reduce the resolution of the input\n",
    "            # While reducing the resolution we also increase the number of channels\n",
    "            # so we also use out_channels here to double the number of channels\n",
    "            self.downsample_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=2, bias=False)\n",
    "            self.downsample_bn = nn.BatchNorm2d(out_channels)\n",
    "    def forward(self, input):\n",
    "        # the forward method calculates the result of the block\n",
    "        conv1_out = self.conv1(input)\n",
    "        bn1_out = self.bn1(conv1_out)\n",
    "        relu1_out = self.relu1(bn1_out)\n",
    "        conv2_out = self.conv2(relu1_out)\n",
    "        bn2_out = self.bn2(conv2_out)\n",
    "        if self.downsample:\n",
    "            # half the width and height of input feature map if the block is downsampling\n",
    "            # this also doubles the channel size\n",
    "            input = self.downsample_conv(input)\n",
    "            input = self.downsample_bn(input)\n",
    "        # skip connection\n",
    "        output = input + bn2_out\n",
    "        return self.relu2(output)\n",
    "\n",
    "\n",
    "class Stem(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sequential = nn.Sequential(\n",
    "            # conv halves width and height and creates 64 channel feature map\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            # maxpool also halves width and height, channels stay the same\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.sequential(x)\n",
    "\n",
    "\n",
    "def get_classification_layer(in_channels, out_channels=1000):\n",
    "    return nn.Sequential(\n",
    "        nn.AdaptiveAvgPool2d(1),\n",
    "        nn.Flatten(),\n",
    "        # standard imagenet has 1000 classes\n",
    "        nn.Linear(in_features=in_channels, out_features=out_channels),\n",
    "    )\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.stage1 = Stem()\n",
    "        self.stage2 = nn.Sequential(\n",
    "            ResidualBlock(64, 64, downsample=False),\n",
    "            ResidualBlock(64, 64, downsample=False)\n",
    "        )\n",
    "        self.stage3 = nn.Sequential(\n",
    "            ResidualBlock(64, 128, downsample=True),\n",
    "            ResidualBlock(128, 128, downsample=False)\n",
    "        )\n",
    "        self.stage4 = nn.Sequential(\n",
    "            ResidualBlock(128, 256, downsample=True),\n",
    "            ResidualBlock(256, 256, downsample=False)\n",
    "        )\n",
    "        self.stage5 = nn.Sequential(\n",
    "            ResidualBlock(256, 512, downsample=True),\n",
    "            ResidualBlock(512, 512, downsample=False)\n",
    "        )\n",
    "        self.classification_layer = get_classification_layer(512)\n",
    "    def forward(self, input):\n",
    "        stage1_output = self.stage1(input)\n",
    "        stage2_output = self.stage2(stage1_output)\n",
    "        stage3_output = self.stage3(stage2_output)\n",
    "        stage4_output = self.stage4(stage3_output)\n",
    "        stage5_output = self.stage5(stage4_output)\n",
    "        classification = self.classification_layer(stage5_output)\n",
    "        return classification\n",
    "\n",
    "summary(ResNet18(), input_size=(3,224,224))"
   ],
   "id": "b1db600ffb46d5f3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ResNet18 Initialization with Pre-Trained Weights",
   "id": "94ce6a2fb77235c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "resnet_model = ResNet18()\n",
    "pretrained_state_dict = resnet18(weights=\"pretrained\").state_dict()\n",
    "new_state_dict = OrderedDict()\n",
    "for key, value in zip(resnet_model.state_dict().keys(), pretrained_state_dict.values()):\n",
    "    new_state_dict[key] = value\n",
    "resnet_model.load_state_dict(new_state_dict)"
   ],
   "id": "50aebb748824dc2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ResNet18 without Prediction Head",
   "id": "6fe563304f106e33"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "resnet_skeleton = nn.Sequential(*list(resnet_model.children())[:-1])\n",
    "summary(resnet_skeleton, input_size=(3,224,224))"
   ],
   "id": "bff64563f94c6adc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Custom ResNet18 with Case-Specific Prediction Head",
   "id": "a8c96b0fca7e7972"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class CustomResNet18(nn.Module):\n",
    "    def __init__(self, skeleton: nn.Module):\n",
    "        super().__init__()\n",
    "        self.skeleton = skeleton\n",
    "        self.classifier = init_model(get_classification_layer(512, 4))\n",
    "    def forward(self, input):\n",
    "        return self.classifier(self.skeleton(input))\n",
    "    def set_requires_grad_skeleton(self, requires_grad: bool = True):\n",
    "        for param in self.skeleton.parameters():\n",
    "            param.requires_grad = requires_grad"
   ],
   "id": "c859a6cc17bbc0d8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Main Model Definition",
   "id": "4f63405e2d615f75"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class MainModel(nn.Module):\n",
    "    def __init__(self, skeleton: nn.Module, lr=1e-3, beta1=0.9, beta2=0.999):\n",
    "        super().__init__()\n",
    "        self.device = dml\n",
    "        self.net = CustomResNet18(skeleton).to(self.device)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.opt = optim.Adam(self.net.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "        self.input = None\n",
    "        self.target = None\n",
    "        self.prediction = None\n",
    "        self.ce_loss = None\n",
    "    def set_requires_grad(self, requires_grad=True):\n",
    "        for p in self.net.parameters():\n",
    "            p.requires_grad = requires_grad\n",
    "    def setup_input(self, data):\n",
    "        self.input = data[0].to(self.device)\n",
    "        self.target = data[1].to(self.device)\n",
    "    def forward(self):\n",
    "        self.prediction = self.net(self.input)\n",
    "    def backward(self):\n",
    "        self.ce_loss = self.criterion(self.prediction, self.target)\n",
    "        self.ce_loss.backward()\n",
    "    def optimize(self):\n",
    "        self.forward()\n",
    "        self.net.train()\n",
    "        self.opt.zero_grad()\n",
    "        self.backward()\n",
    "        self.opt.step()"
   ],
   "id": "95205ed818720e23"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training",
   "id": "d5bfd8235c2fd9f5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Model Initialization",
   "id": "9867d82cab93740a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from_checkpoint = True\n",
    "classifier = MainModel(resnet_skeleton)\n",
    "test_dataloader_iter = iter(test_dataloader)\n",
    "if not from_checkpoint:\n",
    "    last = 0\n",
    "    classifier.net.set_requires_grad_skeleton(False)\n",
    "else:\n",
    "    classifier.net.set_requires_grad_skeleton(True)\n",
    "    checkpoint = None\n",
    "    if checkpoint is None:\n",
    "        last = 0\n",
    "        classifier.load_state_dict(torch.load(os.path.join(rf'model.pth')))\n",
    "    else:\n",
    "        last = checkpoint\n",
    "        classifier.load_state_dict(torch.load(os.path.join(rf'checkpoints/model_chckpt{checkpoint}.pth')))"
   ],
   "id": "30325731bb77f13e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training FUnctions",
   "id": "ad7231727abe5417"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class AverageMeter:\n",
    "    def __init__(self):\n",
    "        self.count, self.avg, self.sum = [0.] * 3\n",
    "    def update(self, val, count=1):\n",
    "        self.count += count\n",
    "        self.sum += count * val\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def create_loss_meters():\n",
    "    loss = AverageMeter()\n",
    "    return {'ce_loss': loss}\n",
    "\n",
    "\n",
    "def update_losses(model, loss_meter_dict, count):\n",
    "    for loss_name, loss_meter in loss_meter_dict.items():\n",
    "        loss = getattr(model, loss_name)\n",
    "        loss_meter.update(loss.item(), count=count)\n",
    "\n",
    "\n",
    "def visualize(model, data, fig = None, ax = None):\n",
    "    labels = np.argmax(data[1], axis=1)\n",
    "    with torch.no_grad():\n",
    "        model.setup_input(data)\n",
    "        model.forward()\n",
    "    predicted = np.argmax(functional.softmax(model.prediction.detach().cpu(), dim=1), axis=1)\n",
    "    c_mat = confusion_matrix(labels, predicted, labels=np.arange(4))\n",
    "    if ax is not None:\n",
    "        ax.clear()\n",
    "    categories = list(map(lambda x: x[1], sorted(list(LABELS.items()), key=lambda x: x[0])))\n",
    "    sns.heatmap(c_mat/c_mat.sum(), \n",
    "                xticklabels=categories, yticklabels=categories,\n",
    "                cmap='Blues',\n",
    "                fmt='.2%',\n",
    "                ax=ax,\n",
    "                cbar=False)\n",
    "    if fig is not None:\n",
    "        fig.show()\n",
    "\n",
    "\n",
    "def log_results(loss_meter_dict):\n",
    "    log = \"\"\n",
    "    for loss_name, loss_meter in loss_meter_dict.items():\n",
    "        log += f\"{loss_name}: {loss_meter.avg:.5f} \"\n",
    "    return log\n",
    "\n",
    "\n",
    "def train_model(model: MainModel, train_dl: DataLoader, test_dl_iter,\n",
    "                epochs: int = 100, display_every: int = 1, save_every: int = 5, \n",
    "                check_point_start: int = 0,\n",
    "                fig = None, ax = None):\n",
    "    check_point = check_point_start\n",
    "    epoch_tqdm = tqdm(range(epochs), \"Epochs\", position=0, leave=True)\n",
    "    for epoch in epoch_tqdm:\n",
    "        loss_meter_dict = create_loss_meters() # function returning a dictionary of objects \n",
    "        batch_tqdm = tqdm(train_dl, \"Batches\", position=1, leave=False)\n",
    "        for data in batch_tqdm:\n",
    "            model.setup_input(data)\n",
    "            model.optimize() # function updating the log objects\n",
    "            update_losses(model, loss_meter_dict, count=32)\n",
    "        batch_tqdm.close()\n",
    "        results = log_results(loss_meter_dict)\n",
    "        epoch_tqdm.set_description(f\"{results}\")\n",
    "        if not (epoch + 1) % display_every:\n",
    "            visualize(model, next(test_dl_iter), fig, ax) # function displaying the model's outputs\n",
    "        if not (epoch + 1) % save_every:\n",
    "            torch.save(model.state_dict(), os.path.join(rf'checkpoints/model_chckpt{check_point}.pth'))\n",
    "            check_point = check_point + 1\n",
    "    return check_point"
   ],
   "id": "16607dd7520eaab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Validation Set Performance Supervision During Training",
   "id": "e73c6cc6cb7fcf5f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## Training Supervision\n",
    "if 'train_figure' in globals():\n",
    "    plt.close('Test Validation')\n",
    "train_figure = plt.figure(num='Test Validation', figsize=(8,8))\n",
    "train_ax = train_figure.subplots(1,1)\n",
    "train_figure.show()"
   ],
   "id": "b5ca85f680832a0f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training",
   "id": "6b9105b9e5bac054"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Training\n",
    "# Warning: carefully set last in order to avoid loosing checkpoints\n",
    "N_epochs = 10\n",
    "Save_Every_N_epochs = 10\n",
    "Show_Every_N_epochs = 1\n",
    "last = train_model(classifier, train_dataloader, test_dataloader_iter, \n",
    "                   N_epochs, Show_Every_N_epochs, Save_Every_N_epochs,\n",
    "                   last, train_figure, train_ax)"
   ],
   "id": "a99006efa4cfe06f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evaluation",
   "id": "6feb3fc60659787c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Checkpoint Evaluation",
   "id": "13be8f4ca6939bbd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Finding the best-performing checkpoint in the validation set\n",
    "validation_data = test_dataset_handler[:]\n",
    "scores = []\n",
    "start_checkpoint = 6\n",
    "end_checkpoint = 9\n",
    "for checkpoint in range(start_checkpoint, end_checkpoint + 1):\n",
    "    inference_classifier = MainModel(resnet_skeleton)\n",
    "    inference_classifier.load_state_dict(torch.load(os.path.join(rf'checkpoints/model_chckpt{checkpoint}.pth')))\n",
    "    with torch.no_grad():\n",
    "        inference_classifier.setup_input(validation_data)\n",
    "        inference_classifier.forward()\n",
    "    predicted = np.argmax(functional.softmax(inference_classifier.prediction.detach().cpu(), dim=1), axis=1)\n",
    "    labels = np.argmax(validation_data[1], axis=1)\n",
    "    score = f1_score(labels, predicted, average='weighted')\n",
    "    scores.append((checkpoint, score))"
   ],
   "id": "7d34ea948ad5818d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Defining the best checkpoint\n",
    "best = scores[0]\n",
    "for score in scores[1:]:\n",
    "    if score[1] > best[1]:\n",
    "        best = score\n",
    "print(f'Best performance observed at validation set is found for checkpoint {best[0]}, with an score of {best[1]:.2f}.')"
   ],
   "id": "1ccec4a7778926d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Best Model Results",
   "id": "11ec97c8f97120c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load checkpoint and get predictions\n",
    "best_classifier = MainModel(resnet_skeleton)\n",
    "best_classifier.load_state_dict(torch.load(os.path.join(rf'checkpoints/model_chckpt{best[0]}.pth')))\n",
    "with torch.no_grad():\n",
    "    best_classifier.setup_input(validation_data)\n",
    "    best_classifier.forward()\n",
    "predicted = np.argmax(functional.softmax(best_classifier.prediction.detach().cpu(), dim=1), axis=1)\n",
    "labels = np.argmax(validation_data[1], axis=1)"
   ],
   "id": "a184d9201b3926d2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Function to plot the confusion matrix\n",
    "def plot_confusion_matrix(ground_truth, predictions, fig = None, ax = None):\n",
    "    c_mat = confusion_matrix(ground_truth, predictions, labels=np.arange(4)).astype(np.float64)\n",
    "    row_sums = c_mat.sum(axis=1)\n",
    "    for index in range(c_mat.shape[0]):\n",
    "        c_mat[index] /= row_sums[index]\n",
    "    if ax is not None:\n",
    "        ax.clear()\n",
    "    categories = list(map(lambda x: x[1], sorted(list(LABELS.items()), key=lambda x: x[0])))\n",
    "    sns.heatmap(c_mat, \n",
    "                xticklabels=categories, yticklabels=categories,\n",
    "                cmap='Blues',\n",
    "                fmt='.2%',\n",
    "                ax=ax,\n",
    "                cbar=False,\n",
    "                annot=True)\n",
    "    if fig is not None:\n",
    "        fig.tight_layout()\n",
    "        fig.show()"
   ],
   "id": "af936d6ae29f3abc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Show Confusion Matrix\n",
    "if 'fig_results' in globals():\n",
    "    plt.close('Results')\n",
    "fig_results = plt.figure(num='Results', figsize=(7,7))\n",
    "ax_results = fig_results.subplots(1,1)\n",
    "fig_results.suptitle('Results', fontsize=14, fontweight='bold')\n",
    "fig_results.supxlabel('Model Predictions')\n",
    "fig_results.supylabel('Ground Truth')\n",
    "plot_confusion_matrix(labels, predicted, fig_results, ax_results)"
   ],
   "id": "cbcb273149912b0e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Show Report\n",
    "target_names = list(map(lambda x: x[1], sorted(list(LABELS.items()), key=lambda x: x[0])))\n",
    "report = classification_report(labels, predicted, target_names=target_names)\n",
    "score = f1_score(labels, predicted, average='weighted')\n",
    "accuracy = accuracy_score(labels, predicted)\n",
    "recall = recall_score(labels, predicted, average='weighted')\n",
    "precision = precision_score(labels, predicted, average='weighted')"
   ],
   "id": "626c637346d44f89"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(report)",
   "id": "e2797f9a17f7e9a5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(f'Model results:\\nAccuracy: {accuracy:.2f}\\nRecall: {recall:.2f}\\nPrecision: {precision:.2f}')",
   "id": "80a691279a6576cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Save checkpoint as the deployment model\n",
    "torch.save(best_classifier.state_dict(), os.path.join(rf'model.pth'))"
   ],
   "id": "804c295a0656f328"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Run this cell when the system's memory is running low\n",
    "gc.collect()\n",
    "clear_output()"
   ],
   "id": "a6237b35e03ef11e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
